视频容器里，放置的是压缩后的图像数据。那么播放器播放，就需要先解压缩成图像，再放到屏幕上。所以，播放器的两个核心功能，一个是解码，一个是显示。



Android系统，已经在底层我们打通了一条MediaPlayer到SurfaceView的数据通路，那就是Surface。 当SurfaceView被创建完成，就可以通过它的SurfaceHolder获取到它的Surface。把Surface传递给MediaPlayer，MediaPlayer解码的数据就会源源不断地输送到SurfaceView里。MediaPlayer有节奏地往Surface输入解码数据，SurfaceView会相应有节奏把Surface里的数据显示到屏幕上。

这种实现方式，解码和显示分别在两个对象中，可以分别控制。但是，我们无法控制它们的数据通路。要牢牢控制每一帧的数据，就要使用下面这种实现。

[android视频系列：视频解码篇--android上视频播放的实现](https://cloud.tencent.com/developer/article/1035435?from=15425)



## 3. 使用MediaPlayer和GLSurfaceView播放视频

GLSurfaceView继承自SurfaceView，它实现了把opengl的渲染结果，绘制到给定的Surface里，进而可以显示在屏幕上。

它的几个主要特点：

- 内部管理了一个EGL display，用于把opengl渲染的结果输出到Surface里。
- 提供Renderer接口，使用者可以通过实现这个接口，来控制opengl渲染的行为和内容。
- opengl渲染工作在特定一个线程里，与UI线程解耦开来。
- 支持on-demand和continuous两种渲染模式。

让我们来看看，如何使用GLSurfaceView来实现视频的播放。

首先创建好GLSurfaceView。











# HLS视频点播&直播初探

- RTMP（Real Time Messaging Protocol）
- HLS（HTTP Live Streaming） 其中`RTMP`是Adobe开发的协议，无法在iPhone中兼容，故目前兼容最好的就是HLS协议了。

HTTP Live Streaming（HLS）是苹果公司实现的基于HTTP的流媒体传输协议，可实现流媒体的直播和点播。原理上是将视频流分片成一系列HTTP下载文件。所以，HLS比RTMP有较高的延迟。

### 前端播放HLS

- Native支持 
  1. Android 3.0+
  2. iOS 3.0+
- flash支持 
  1. Flowplayer（GPL `×`）
  2. GrindPlayer（MIT）
  3. video-js-swf（Apache License 2.0）
  4. MediaElement.js（MIT）
  5. clappr（BSD IE10+ `×`）